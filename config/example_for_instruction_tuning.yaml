seed_everything: 42
trainer:
  strategy: 
    class_path: EnhancedDeepSpeedStrategy
    init_args:
      stage: 3
      logging_level: ERROR
  precision: 16-true
  logger:
    class_path: EnhancedWandbLogger
    init_args:
      name: example_for_instruction_tuning
      save_dir: logs
      project: taide-cp-example
      tags:
        - sft
        - fp16
      save_code: true
  callbacks:
    - class_path: LearningRateMonitor
    - class_path: ModelCheckpoint
      init_args:
        save_on_train_epoch_end: true
        save_top_k: -1
  max_epochs: 3
  min_epochs: 1
  max_steps: -1
  min_steps: null
  val_check_interval: null
  check_val_every_n_epoch: 1
  accumulate_grad_batches: 1
  gradient_clip_val: 1.0
  gradient_clip_algorithm: norm
model:
  class_path: LitCausalLM
  init_args:
    config:
      model_path: <MODEL_PATH>
      optimizer_config:
        lr: 1e-5
        betas:
          - 0.9
          - 0.95
        eps: 1e-5
        weight_decay: 1e-1
        lr_scheduler_type: cosine
        num_warmup_steps: 0
        min_lr_factor: 0.1
      # patchers:
      #   - class_path: LlamaOptimizationPatcher
      #   - class_path: LlamaFP16Patcher
data:
  class_path: DataModuleForInstructionTuning
  init_args:
    config:
      dataset_kwargs:
        path: TLLM/ft-balance-mixed
      max_length: 4096
      chat_template: llama
      concat_method: GROUP_BY_LENGTH
      pad_to_multiple_of: 8
      batch_size: 4
      num_proc: 4
      num_workers: 4
# float32_matmul_precision: medium
# logging_level: DEBUG
# save_config: false
