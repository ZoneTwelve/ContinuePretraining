import functools
import json
from pathlib import Path
from typing import Any, Optional, cast

import fire
import torch
from accelerate import init_empty_weights
from transformers import GenerationConfig, PreTrainedModel

from taide_cp.models import AutoConfig, AutoModelForCausalLM, AutoTokenizer
from taide_cp.utils import rsetattr


def load_checkpoint(model: PreTrainedModel, checkpoint_path: str, device: torch.device):
    state_dict = torch.load(checkpoint_path, device)
    for k in list(state_dict.keys()):
        state_dict[k.removeprefix('_forward_module.model.')] = state_dict.pop(k)
    
    for k, v in state_dict.items():
        rsetattr(model, k, torch.nn.Parameter(v))
    
    model.to(device)
    model.tie_weights()

def chunk(lst: list, n: int):
    for i in range(0, len(lst), n):
        yield lst[i:i + n]

def main(
    model_path: str,
    checkpoint_path: Optional[str] = None,
    output_path: Optional[str] = 'outputs/generation/',
    batch_size: int = 4,
    device: str = 'cuda'
):
    device = torch.device(device)
    tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side='left')

    if 'pad_token' not in tokenizer.special_tokens_map:
        tokenizer.add_special_tokens({'[PAD]': tokenizer.bos_token})

    if checkpoint_path is None:
        config = AutoConfig.from_pretrained(model_path)
        model = AutoModelForCausalLM.from_pretrained(model_path, config=config, device_map=device, torch_dtype='auto', low_cpu_mem_usage=True)
        model = cast(PreTrainedModel, model)
    else:
        config = AutoConfig.from_pretrained(model_path)
        with init_empty_weights():
            model = AutoModelForCausalLM.from_config(config)
        load_checkpoint(model, checkpoint_path, device)

    prompts = [
        'I believe the meaning of life is',
        'Simply put, the theory of relativity states that',
        'Building a website can be done in 10 simple steps:\n',
        'Tweet: "I hate it when my phone battery dies.\n"\nSentiment: Negative\n###\nTweet: "My day has been ğŸ‘"\nSentiment: Positive\n###\nTweet: "This is the link to the article"\nSentiment: Neutral\n###\nTweet: "This new music video was incredibile"\nSentiment:',
        'Translate English to French:\nsea otter => loutre de mer\npeppermint => menthe poivrÃ©e\nplush girafe => girafe peluche\ncheese =>',
        'æˆ‘ç›¸ä¿¡ç”Ÿå‘½çš„æ„ç¾©åœ¨æ–¼',
        'ç°¡å–®åœ°èªªï¼Œç›¸å°è«–æŒ‡å‡º',
        'å»ºç«‹ä¸€å€‹ç¶²ç«™å¯ä»¥é€šé 10 å€‹ç°¡å–®çš„æ­¥é©Ÿå®Œæˆï¼š\n1.',
        'æ¨æ–‡ï¼šâ€œæˆ‘è¨å­æ‰‹æ©Ÿæ²’é›»äº†ã€‚â€œ\næƒ…ç·’ï¼šè² é¢\n###\næ¨æ–‡ï¼šâ€œæˆ‘çš„ä¸€å¤©æ˜¯ğŸ‘â€\næƒ…ç·’ï¼šæ­£é¢\n###\næ¨æ–‡ï¼šâ€œé€™æ˜¯æ–‡ç« çš„éˆæ¥â€\næƒ…ç·’ï¼šä¸­æ€§\n###\næ¨æ–‡ï¼šâ€œé€™å€‹æ–°çš„éŸ³æ¨‚å½±ç‰‡å¤ªæ£’äº†â€\næƒ…ç·’ï¼š',
        'å°‡è‹±æ–‡ç¿»è­¯æˆä¸­æ–‡ï¼š\nsea otter => æµ·çº\npeppermint => è–„è·\nplush girafe => æ¯›çµ¨é•·é ¸é¹¿\ncheese =>',
        "å¹«æˆ‘ç¿»è­¯ä¸‹åˆ—å¥å­æˆä¸­æ–‡ï¼š I am a Master student studying computer science, and my research focus is NLP.",
        "å¦‚æœå¿ƒæƒ…ä¸å¥½è¦æ€éº¼è¾¦\nä½ å¯ä»¥",
        "å‘Šè¨´æˆ‘ä¸‰ç¨®è³ºéŒ¢çš„æ–¹å¼\n1. ",
        "è«‹è§£ä¸‹åˆ—ä¸€å…ƒä¸€æ¬¡æ–¹ç¨‹å¼: x + 3 = 10, x = ",
        "å°ç£æœ€é«˜çš„å»ºç¯‰ç‰©æ˜¯",
        "å°ç£æœ€å¥½çš„å¤§å­¸æ˜¯",
        "I like chocolate = æˆ‘å–œæ­¡å·§å…‹åŠ›\nI wish you a nice day =",
        "ä»¥ã€Œæˆ‘ã€ç‚ºä¸»è§’ï¼Œå¯«å‡ºä¸€å€‹é—œæ–¼å¦‚ä½•ç²å–æœ‰æ•ˆå­¸ç¿’ç­–ç•¥çš„æ•…äº‹ã€‚\næˆ‘",
        "ä½ å¾å°å°±å¤¢æƒ³æˆç‚ºä¸€åä½œå®¶ï¼Œçµ‚æ–¼æœ‰ä¸€å¤©ä½ çš„å¤¢æƒ³æˆçœŸäº†ã€‚ä½ å¯«ä¸‹äº†ä»¤äººé©šè‰·çš„å°èªªä½œå“ï¼Œä½†ä½ çš„çˆ¶æ¯åš´å²çœ‹å¾…ä½ å¾äº‹é€™æ¢è·æ¥­é“è·¯ï¼Œèªç‚ºä½ å°‡ä¾†ä¸ç©©å®šï¼Œä¸”æ²’æœ‰å‰é€”ã€‚å¯«ä¸‹ä½ å’Œçˆ¶æ¯é–“çˆ­åµçš„æƒ…æ™¯ã€‚\næˆ‘"
    ]

    generation_config = GenerationConfig(
        max_new_tokens=32,
        num_beams=4,
        temperature=0.1,
        top_p=0.1,
        pad_token_id=tokenizer.pad_token_id,
        eos_token_id=tokenizer.eos_token_id,
    )

    with torch.autocast('cuda', dtype=model.dtype):
        output_text = []
        for batch in chunk(prompts, batch_size):
            x = tokenizer(batch, return_tensors='pt', padding=True, return_token_type_ids=False).to(model.device)
            l = x['input_ids'].size(1)
            x = model.generate(**x, generation_config=generation_config)
            x = tokenizer.batch_decode(x[:, l:], skip_special_tokens=True, clean_up_tokenization_spaces=True)
            output_text += x

    for p, o in zip(prompts, output_text):
        print(p, end=' ||| ')
        print(o)
        print('=' * 100)

    if output_path:
        output_path: Path = Path(output_path)
        if output_path.is_dir():
            output_path = output_path / (Path(model_path).name + '.generation.json')

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump({
                'generation_config': generation_config.to_diff_dict(),
                'prompts': prompts,
                'outputs': output_text,
            }, f, ensure_ascii=False, indent=True)

if __name__ == '__main__':
    fire.Fire(main)
